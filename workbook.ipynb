{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data from txt\n",
    "with open(\"2024\\Day_02\\data.txt\", \"r\") as f:\n",
    "    data = f.read().strip().split(\"\\n\")\n",
    "int_data = [list(map(int, re.findall(\"\\\\d+\", x))) for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day 2 help from Reddit...\n",
    "def safe1(levels):\n",
    "    return all(1 <= abs(n1 - n2) <= 3 for n1, n2 in zip(levels, levels[1:])) and (\n",
    "        levels == sorted(levels) or levels == sorted(levels)[::-1]\n",
    "    )\n",
    "\n",
    "\n",
    "def safe2(levels):\n",
    "    return any(safe1(levels[:i] + levels[i + 1 :]) for i in range(len(levels)))\n",
    "\n",
    "\n",
    "# Part 1\n",
    "print(sum(safe1(levels) for levels in int_data))\n",
    "\n",
    "# Part 2\n",
    "print(sum(safe2(levels) for levels in int_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Example patterns\n",
    "pattern1 = r\"start\"\n",
    "pattern2 = r\"middle\"\n",
    "pattern3 = r\"reset\"\n",
    "\n",
    "# Example string\n",
    "text = \"start some text middle reset start middle start reset middle\"\n",
    "\n",
    "# Compile patterns\n",
    "p1 = re.compile(pattern1)\n",
    "p2 = re.compile(pattern2)\n",
    "p3 = re.compile(pattern3)\n",
    "\n",
    "# Initialize flags and results\n",
    "search_for_p2 = False\n",
    "results = []\n",
    "\n",
    "# Iterate through all matches in the string\n",
    "for match in re.finditer(rf\"{pattern1}|{pattern2}|{pattern3}\", text):\n",
    "    match_text = match.group()\n",
    "\n",
    "    if p1.fullmatch(match_text):  # Found pattern1\n",
    "        search_for_p2 = True\n",
    "    elif p3.fullmatch(match_text):  # Found pattern3\n",
    "        search_for_p2 = False\n",
    "    elif p2.fullmatch(match_text) and search_for_p2:  # Found pattern2 while searching for it\n",
    "        results.append(match_text)\n",
    "        search_for_p2 = False  # Stop searching for pattern2 until next pattern1\n",
    "\n",
    "# Output results\n",
    "print(\"Pattern2 Matches:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_words_multiple(grid, words):\n",
    "    rows = len(grid)\n",
    "    cols = len(grid[0])\n",
    "    \n",
    "    # Define 8 possible directions (row_offset, col_offset)\n",
    "    directions = [\n",
    "        (0, 1),   # Right\n",
    "        (0, -1),  # Left\n",
    "        (1, 0),   # Down\n",
    "        (-1, 0),  # Up\n",
    "        (1, 1),   # Down-Right\n",
    "        (-1, -1), # Up-Left\n",
    "        (1, -1),  # Down-Left\n",
    "        (-1, 1),  # Up-Right\n",
    "    ]\n",
    "    \n",
    "    def is_valid(x, y):\n",
    "        \"\"\"Check if coordinates are within bounds.\"\"\"\n",
    "        return 0 <= x < rows and 0 <= y < cols\n",
    "    \n",
    "    def search_from(x, y, word):\n",
    "        \"\"\"Search for a word starting from (x, y) in all directions.\"\"\"\n",
    "        positions = []\n",
    "        for row_offset, col_offset in directions:\n",
    "            nx, ny = x, y\n",
    "            match = True\n",
    "            for char in word:\n",
    "                if not is_valid(nx, ny) or grid[nx][ny] != char:\n",
    "                    match = False\n",
    "                    break\n",
    "                nx += row_offset\n",
    "                ny += col_offset\n",
    "            if match:\n",
    "                # If a match is found, record the start position and direction\n",
    "                positions.append((x, y, row_offset, col_offset))\n",
    "        return positions\n",
    "    \n",
    "    # Result dictionary\n",
    "    found_words = {word: [] for word in words}\n",
    "    \n",
    "    # Search for each word\n",
    "    for word in words:\n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                if grid[i][j] == word[0]:\n",
    "                    occurrences = search_from(i, j, word)\n",
    "                    if occurrences:\n",
    "                        found_words[word].extend(occurrences)\n",
    "    \n",
    "    return found_words\n",
    "\n",
    "# Example word search grid\n",
    "word_search = [\n",
    "    ['c', 'a', 't', 'f'],\n",
    "    ['b', 'g', 'd', 'o'],\n",
    "    ['x', 'y', 'z', 'g'],\n",
    "    ['d', 'o', 'g', 'a']\n",
    "]\n",
    "\n",
    "# Words to find\n",
    "word_list = [\"cat\", \"dog\", \"god\", \"fat\", \"xyz\"]\n",
    "\n",
    "# Solve word search\n",
    "results = find_words_multiple(word_search, word_list)\n",
    "\n",
    "# Print results\n",
    "for word, positions in results.items():\n",
    "    if positions:\n",
    "        print(f\"Word '{word}' found {len(positions)} times at positions: {positions}\")\n",
    "    else:\n",
    "        print(f\"Word '{word}' not found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_diagonal_x_for_word(grid, word):\n",
    "    rows = len(grid)\n",
    "    cols = len(grid[0])\n",
    "\n",
    "    # Define diagonal directions\n",
    "    directions = [\n",
    "        (1, 1),   # Down-Right\n",
    "        (-1, -1), # Up-Left\n",
    "        (1, -1),  # Down-Left\n",
    "        (-1, 1),  # Up-Right\n",
    "    ]\n",
    "\n",
    "    def is_valid(x, y):\n",
    "        \"\"\"Check if coordinates are within bounds.\"\"\"\n",
    "        return 0 <= x < rows and 0 <= y < cols\n",
    "\n",
    "    def search_from(x, y, word, direction):\n",
    "        \"\"\"Search for a word starting from (x, y) in a single diagonal direction.\"\"\"\n",
    "        row_offset, col_offset = direction\n",
    "        nx, ny = x, y\n",
    "        path = []\n",
    "        for char in word:\n",
    "            if not is_valid(nx, ny) or grid[nx][ny] != char:\n",
    "                return None  # Invalid path\n",
    "            path.append((nx, ny))\n",
    "            nx += row_offset\n",
    "            ny += col_offset\n",
    "        return path\n",
    "\n",
    "    def check_x_shape(path1, path2):\n",
    "        \"\"\"Check if two paths form an 'X' by intersecting at their midpoints.\"\"\"\n",
    "        mid1 = path1[len(path1) // 2]  # Middle of first path\n",
    "        mid2 = path2[len(path2) // 2]  # Middle of second path\n",
    "        return mid1 == mid2  # Intersection at the same point\n",
    "\n",
    "    # Search the grid for the word forming an 'X'\n",
    "    x_shapes = []\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            if grid[i][j] == word[0]:  # Starting point must match the first letter\n",
    "                # Search in all diagonal directions\n",
    "                for dir1 in directions:\n",
    "                    path1 = search_from(i, j, word, dir1)\n",
    "                    if path1:\n",
    "                        # For the second diagonal, pick the opposite direction\n",
    "                        for dir2 in directions:\n",
    "                            if dir1 == dir2 or dir1 == (-dir2[0], -dir2[1]):\n",
    "                                continue  # Skip same or reverse directions\n",
    "                            path2 = search_from(i, j, word, dir2)\n",
    "                            if path2 and check_x_shape(path1, path2):\n",
    "                                x_shapes.append((path1, path2))\n",
    "\n",
    "    return x_shapes\n",
    "\n",
    "# Example word search grid\n",
    "word_search = [\n",
    "    ['M', 'M', 'M', 'S', 'X', 'X', 'M', 'A', 'S', 'M'],\n",
    "    ['M', 'S', 'A', 'M', 'X', 'M', 'S', 'M', 'S', 'A'],\n",
    "    ['A', 'M', 'X', 'S', 'X', 'M', 'A', 'A', 'M', 'M'],\n",
    "    ['M', 'S', 'A', 'M', 'A', 'S', 'M', 'S', 'M', 'X'],\n",
    "    ['X', 'M', 'A', 'S', 'A', 'M', 'X', 'A', 'M', 'M'],\n",
    "    ['X', 'X', 'A', 'M', 'M', 'X', 'X', 'A', 'M', 'A'],\n",
    "    ['S', 'M', 'S', 'M', 'S', 'A', 'S', 'X', 'S', 'S'],\n",
    "    ['S', 'A', 'X', 'A', 'M', 'A', 'S', 'A', 'A', 'A'],\n",
    "    ['M', 'A', 'M', 'M', 'M', 'X', 'M', 'M', 'M', 'M'],\n",
    "    ['M', 'X', 'M', 'X', 'A', 'X', 'M', 'A', 'S', 'X'],\n",
    "]\n",
    "\n",
    "# Word to find\n",
    "word = \"MAS\"\n",
    "\n",
    "# Solve word search for 'X' shape\n",
    "results = find_diagonal_x_for_word(word_search, word)\n",
    "\n",
    "# Print results\n",
    "if results:\n",
    "    for paths in results:\n",
    "        path1, path2 = paths\n",
    "        print(f\"Word '{word}' forms an 'X' at intersection with paths:\")\n",
    "        print(f\"  Path1: {path1}\")\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_diagonal_x_for_word(grid, word):\n",
    "    rows = len(grid)\n",
    "    cols = len(grid[0])\n",
    "\n",
    "    # Define diagonal directions\n",
    "    directions = [\n",
    "        (1, 1),   # Down-Right\n",
    "        (-1, -1), # Up-Left\n",
    "        (1, -1),  # Down-Left\n",
    "        (-1, 1),  # Up-Right\n",
    "    ]\n",
    "\n",
    "    def is_valid(x, y):\n",
    "        \"\"\"Check if coordinates are within bounds.\"\"\"\n",
    "        return 0 <= x < rows and 0 <= y < cols\n",
    "\n",
    "    def search_from(x, y, word, direction):\n",
    "        \"\"\"Search for a word starting from (x, y) in a single diagonal direction.\"\"\"\n",
    "        row_offset, col_offset = direction\n",
    "        nx, ny = x, y\n",
    "        path = []\n",
    "        for char in word:\n",
    "            if not is_valid(nx, ny) or grid[nx][ny] != char:\n",
    "                return None  # Invalid path\n",
    "            path.append((nx, ny))\n",
    "            nx += row_offset\n",
    "            ny += col_offset\n",
    "        return path\n",
    "\n",
    "    def verify_x_shape(paths):\n",
    "        \"\"\"Verify if any two paths form an 'X' by intersecting at their midpoints.\"\"\"\n",
    "        midpoints = {tuple(path[len(path) // 2]) for path in paths}  # Extract all midpoints\n",
    "        # Check if any midpoint is shared between two paths\n",
    "        result = []\n",
    "        for i in range(len(paths)):\n",
    "            for j in range(i + 1, len(paths)):\n",
    "                mid1 = paths[i][len(paths[i]) // 2]\n",
    "                mid2 = paths[j][len(paths[j]) // 2]\n",
    "                if mid1 == mid2:  # Shared midpoint\n",
    "                    result.append((paths[i], paths[j]))\n",
    "        return result\n",
    "\n",
    "    # Search for all diagonal occurrences of the word\n",
    "    all_paths = []\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            if grid[i][j] == word[0]:  # Starting point must match the first letter\n",
    "                for direction in directions:\n",
    "                    path = search_from(i, j, word, direction)\n",
    "                    if path:\n",
    "                        all_paths.append(path)\n",
    "\n",
    "    # Verify which paths form 'X' shapes\n",
    "    x_shapes = verify_x_shape(all_paths)\n",
    "    return x_shapes\n",
    "\n",
    "\n",
    "# Example word search grid\n",
    "word_search = [\n",
    "    ['M', 'M', 'M', 'S', 'X', 'X', 'M', 'A', 'S', 'M'],\n",
    "    ['M', 'S', 'A', 'M', 'X', 'M', 'S', 'M', 'S', 'A'],\n",
    "    ['A', 'M', 'X', 'S', 'X', 'M', 'A', 'A', 'M', 'M'],\n",
    "    ['M', 'S', 'A', 'M', 'A', 'S', 'M', 'S', 'M', 'X'],\n",
    "    ['X', 'M', 'A', 'S', 'A', 'M', 'X', 'A', 'M', 'M'],\n",
    "    ['X', 'X', 'A', 'M', 'M', 'X', 'X', 'A', 'M', 'A'],\n",
    "    ['S', 'M', 'S', 'M', 'S', 'A', 'S', 'X', 'S', 'S'],\n",
    "    ['S', 'A', 'X', 'A', 'M', 'A', 'S', 'A', 'A', 'A'],\n",
    "    ['M', 'A', 'M', 'M', 'M', 'X', 'M', 'M', 'M', 'M'],\n",
    "    ['M', 'X', 'M', 'X', 'A', 'X', 'M', 'A', 'S', 'X'],\n",
    "]\n",
    "\n",
    "# Word to find\n",
    "word = \"MAS\"\n",
    "\n",
    "# Solve word search for 'X' shape\n",
    "results = find_diagonal_x_for_word(word_search, word)\n",
    "\n",
    "# Print results\n",
    "if results:\n",
    "    print(f\"\\nTotal 'X' shapes found: {len(results)}\")\n",
    "    for paths in results:\n",
    "        path1, path2 = paths\n",
    "        print(f\"Word '{word}' forms an 'X' at intersection with paths:\")\n",
    "        print(f\"  Path1: {path1}\")\n",
    "        print(f\"  Path2: {path2}\")    \n",
    "else:\n",
    "    print(f\"Word '{word}' does not form an 'X' in the grid.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, deque\n",
    "\n",
    "def validate_order(rules, order):\n",
    "    # Build a graph and calculate in-degrees\n",
    "    graph = defaultdict(list)\n",
    "    in_degree = defaultdict(int)\n",
    "    \n",
    "    # Build the graph from rules\n",
    "    for a, b in rules:\n",
    "        graph[a].append(b)\n",
    "        in_degree[b] += 1\n",
    "        in_degree.setdefault(a, 0)  # Ensure all nodes are in the in-degree map\n",
    "    \n",
    "    # Perform topological sort using Kahn's algorithm\n",
    "    queue = deque([node for node in in_degree if in_degree[node] == 0])\n",
    "    topological_order = []\n",
    "    \n",
    "    while queue:\n",
    "        current = queue.popleft()\n",
    "        topological_order.append(current)\n",
    "        for neighbor in graph[current]:\n",
    "            in_degree[neighbor] -= 1\n",
    "            if in_degree[neighbor] == 0:\n",
    "                queue.append(neighbor)\n",
    "    \n",
    "    # If the graph has a cycle, the topological order won't include all nodes\n",
    "    if len(topological_order) != len(in_degree):\n",
    "        raise ValueError(\"The rules contain a cycle, so no valid order exists.\")\n",
    "    \n",
    "    # Check if the input order respects the topological order\n",
    "    position = {node: i for i, node in enumerate(topological_order)}\n",
    "    for i in range(len(order) - 1):\n",
    "        if position[order[i]] > position[order[i + 1]]:\n",
    "            return False  # Order is invalid\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Example usage\n",
    "rules = [('A', 'B'), ('B', 'C'), ('C', 'D')]\n",
    "order = ['A', 'B', 'C', 'D']  # Valid order\n",
    "invalid_order = ['A', 'C', 'B', 'D']  # Invalid order\n",
    "\n",
    "print(validate_order(rules, order))         # Output: True\n",
    "print(validate_order(rules, invalid_order))  # Output: False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['47|53\\n97|13\\n97|61\\n97|47\\n75|29\\n61|13\\n75|53\\n29|13\\n97|29\\n53|29\\n61|53\\n97|53\\n61|29\\n47|13\\n75|47\\n97|75\\n47|61\\n75|61\\n47|29\\n75|13\\n53|13', '75,47,61,53,29\\n97,61,53,29,13\\n75,29,13\\n75,97,47,61,53\\n61,13,29\\n97,13,75,29,47']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "the number of columns changed from 5 to 3 at row 3; use `usecols` to select a subset and avoid this error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Parse each section using pandas\u001b[39;00m\n\u001b[0;32m     10\u001b[0m data1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mloadtxt(sections[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msplitlines(), delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m---> 11\u001b[0m data2 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadtxt\u001b[49m\u001b[43m(\u001b[49m\u001b[43msections\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitlines\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData Set 1:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(data1)\n",
      "File \u001b[1;32mc:\\Users\\Madz\\Documents\\advent-of-code\\.venv\\Lib\\site-packages\\numpy\\lib\\_npyio_impl.py:1397\u001b[0m, in \u001b[0;36mloadtxt\u001b[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[0;32m   1394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(delimiter, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1395\u001b[0m     delimiter \u001b[38;5;241m=\u001b[39m delimiter\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1397\u001b[0m arr \u001b[38;5;241m=\u001b[39m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelimiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1398\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiplines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1399\u001b[0m \u001b[43m            \u001b[49m\u001b[43munpack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munpack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mndmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1400\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1402\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n",
      "File \u001b[1;32mc:\\Users\\Madz\\Documents\\advent-of-code\\.venv\\Lib\\site-packages\\numpy\\lib\\_npyio_impl.py:1036\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[0;32m   1033\u001b[0m     data \u001b[38;5;241m=\u001b[39m _preprocess_comments(data, comments, encoding)\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read_dtype_via_object_chunks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1036\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43m_load_from_filelike\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1037\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelimiter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1038\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimaginary_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimaginary_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1039\u001b[0m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiplines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiplines\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1040\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1041\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilelike\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilelike\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1042\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbyte_converters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbyte_converters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1045\u001b[0m     \u001b[38;5;66;03m# This branch reads the file into chunks of object arrays and then\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m     \u001b[38;5;66;03m# casts them to the desired actual dtype.  This ensures correct\u001b[39;00m\n\u001b[0;32m   1047\u001b[0m     \u001b[38;5;66;03m# string-length and datetime-unit discovery (like `arr.astype()`).\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m     \u001b[38;5;66;03m# Due to chunking, certain error reports are less clear, currently.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filelike:\n",
      "\u001b[1;31mValueError\u001b[0m: the number of columns changed from 5 to 3 at row 3; use `usecols` to select a subset and avoid this error"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Read the file into memory\n",
    "with open('2024\\\\Day_05\\\\test_data.txt', 'r') as file:\n",
    "    sections = file.read().strip().split(\"\\n\\n\")\n",
    "print(sections)\n",
    "# Parse each section using pandas\n",
    "data1 = np.loadtxt(sections[0].splitlines(), delimiter=\"|\").tolist()\n",
    "data2 = np.loadtxt(sections[1].splitlines(), delimiter=\",\").tolist()\n",
    "\n",
    "print(\"Data Set 1:\")\n",
    "print(data1)\n",
    "\n",
    "print(\"Data Set 2:\")\n",
    "print(data2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
